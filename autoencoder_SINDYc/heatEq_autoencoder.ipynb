{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import pysindy\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, x_rom_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        h1_nodes = max(x_rom_dim, (x_dim + x_rom_dim) // 2)     # max(3, 23//2) = 11\n",
    "        h2_nodes = max(x_rom_dim, (x_dim + x_rom_dim) // 2)     # max(3, 23//2) = 11\n",
    "        h3_nodes = max(x_rom_dim, (x_dim + x_rom_dim) // 4)     # max(3, 23//4) = 5\n",
    "        self.input = nn.Linear(x_dim, h1_nodes)\n",
    "        self.h1 = nn.Linear(h1_nodes, h2_nodes)\n",
    "        self.h2 = nn.Linear(h2_nodes, h3_nodes)\n",
    "        self.h3 = nn.Linear(h3_nodes, x_rom_dim)\n",
    "        nn.init.kaiming_uniform_(self.input.weight)\n",
    "        nn.init.kaiming_uniform_(self.h1.weight)\n",
    "        nn.init.kaiming_uniform_(self.h2.weight)\n",
    "        nn.init.kaiming_uniform_(self.h3.weight)\n",
    "    def forward(self, x_in):\n",
    "        xe1 = F.elu(self.input(x_in))\n",
    "        xe2 = F.elu(self.h1(xe1))\n",
    "        xe3 = F.elu(self.h2(xe2))\n",
    "        x_rom_out = (self.h3(xe3))\n",
    "\n",
    "        # xe1 = F.leaky_relu(self.input(x_in))\n",
    "        # xe2 = F.leaky_relu(self.h1(xe1))\n",
    "        # xe3 = F.leaky_relu(self.h2(xe2))\n",
    "        # x_rom_out = (self.h3(xe3))\n",
    "        return x_rom_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, x_rom_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        h1_nodes = max(x_rom_dim, (x_dim + x_rom_dim) // 4)     # max(3, 23//4) = 5\n",
    "        h2_nodes = max(x_rom_dim, (x_dim + x_rom_dim) // 2)     # max(3, 23//2) = 11\n",
    "        h3_nodes = max(x_rom_dim, (x_dim + x_rom_dim) // 2)     # max(3, 23//2) = 11\n",
    "        self.input = nn.Linear(x_rom_dim, h1_nodes)\n",
    "        self.h1 = nn.Linear(h1_nodes, h2_nodes)\n",
    "        self.h2 = nn.Linear(h2_nodes, h3_nodes)\n",
    "        self.h3 = nn.Linear(h3_nodes, x_dim)\n",
    "        nn.init.kaiming_uniform_(self.input.weight)\n",
    "        nn.init.kaiming_uniform_(self.h1.weight)\n",
    "        nn.init.kaiming_uniform_(self.h2.weight)\n",
    "        nn.init.kaiming_uniform_(self.h3.weight)\n",
    "    def forward(self, x_rom_in):\n",
    "        xe1 = F.elu(self.input(x_rom_in))\n",
    "        xe2 = F.elu(self.h1(xe1))\n",
    "        xe3 = F.elu(self.h2(xe2))\n",
    "        x_full_out = (self.h3(xe3))\n",
    "\n",
    "        # xe1 = F.leaky_relu(self.input(x_rom_in))\n",
    "        # xe2 = F.leaky_relu(self.h1(xe1))\n",
    "        # xe3 = F.leaky_relu(self.h2(xe2))\n",
    "        # x_full_out = (self.h3(xe3))\n",
    "        return x_full_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    def __init__(self, x_dim, x_rom_dim=3):\n",
    "        self.encoder = Encoder(x_dim, x_rom_dim)\n",
    "        self.decoder = Decoder(x_dim, x_rom_dim)\n",
    "        self.scaler_x = preprocessing.MinMaxScaler()\n",
    "    def process_and_normalize_data(self, dataframe):\n",
    "        x = []\n",
    "        for i in range(20):\n",
    "            x.append(dataframe[\"x{}\".format(i)].to_numpy(dtype=np.float32))\n",
    "\n",
    "        # x is fit with n rows and 20 columns, so we need to reshape it to this for transform\n",
    "        # Tranpose x to obtain a 2D array with shape (num_trajectories * time, 20)\n",
    "        x = np.array(x).transpose()\n",
    "        self.scaler_x.fit(x)\n",
    "        x = self.scaler_x.transform(x)\n",
    "        x_tensor = torch.tensor(x)\n",
    "        return x_tensor\n",
    "    def transform_data_without_fit(self, dataframe):\n",
    "        x = []\n",
    "        for i in range(20):\n",
    "            x.append(dataframe[\"x{}\".format(i)].to_numpy(dtype=np.float32))\n",
    "\n",
    "        # x is fit with n rows and 20 columns, so we need to reshape it to this for transform\n",
    "        # Tranpose x to obtain a 2D array with shape (num_trajectories * time, 20)\n",
    "        x = np.array(x).transpose()\n",
    "        x = self.scaler_x.transform(x)\n",
    "        x_tensor = torch.tensor(x)\n",
    "        return x_tensor\n",
    "    def fit(self, dataframe, test_dataframe):\n",
    "        x = self.process_and_normalize_data(dataframe)\n",
    "        x_test = self.transform_data_without_fit(test_dataframe)\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        mb_loader = torch.utils.data.DataLoader(x, batch_size=100, shuffle=True)\n",
    "        param_wrapper = nn.ParameterList()\n",
    "        param_wrapper.extend(self.encoder.parameters())\n",
    "        param_wrapper.extend(self.decoder.parameters())\n",
    "        optimizer = optim.SGD(param_wrapper, lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        for epoch in range(2000):\n",
    "            for x_mb in mb_loader:\n",
    "                optimizer.zero_grad()\n",
    "                x_rom_mb = self.encoder(x_mb)\n",
    "                x_full_pred_mb = self.decoder(x_rom_mb)\n",
    "                loss = criterion(x_full_pred_mb, x_mb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Test on the test dataset at this epoch\n",
    "            self.encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            with torch.no_grad():\n",
    "                x_rom = self.encoder(x_test)\n",
    "                x_full_pred = self.decoder(x_rom)\n",
    "                loss = criterion(x_full_pred, x_test)\n",
    "                mae = metrics.mean_absolute_error(x_full_pred, x_test)\n",
    "                mape = metrics.mean_absolute_percentage_error(x_full_pred, x_test)\n",
    "                print(\"Held out test dataset - Epoch {}: MSE = {}, MAE = {}, MAPE = {}\".format(epoch, loss, mae, mape))\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "    def encode(self, dataframe):\n",
    "        print(dataframe)\n",
    "        x = self.transform_data_without_fit(dataframe)\n",
    "        self.encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            x_rom = self.encoder(x)\n",
    "        x_rom = x_rom.numpy()\n",
    "        # x_rom = self.scaler_x.inverse_transform(x_rom)\n",
    "        x_rom_df_cols = []\n",
    "        for i in range(x_rom.shape[1]):\n",
    "            x_rom_df_cols.append(\"x{}_rom\".format(i))\n",
    "        x_rom_df = pd.DataFrame(x_rom, columns=x_rom_df_cols)\n",
    "        print(x_rom_df)\n",
    "        return x_rom_df\n",
    "    def decode(self, x_rom_nparr):\n",
    "        # Expected shape of x_rom_nparr is (x_rom_dim, )\n",
    "        # Reshape to match decoder dimensions\n",
    "        x_rom_nparr = x_rom_nparr.reshape(1, 3)\n",
    "        self.decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            x_decoded = self.decoder(x_rom_nparr)\n",
    "\n",
    "        # Scale x_decoded into x_full\n",
    "        x_decoded = self.scaler_x.inverse_transform(x_decoded)\n",
    "\n",
    "        # Return x_decoded as a numpy array, not pandas dataframe (to the basinhopper)\n",
    "        return x_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as model:\n",
    "        pickled_autoencoder = pickle.load(model)\n",
    "    print(\"Pickle loaded: \" + filename)\n",
    "    return pickled_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_for_svm():\n",
    "    autoencoder = load_pickle(\"heatEq_autoencoder_3dim_lr001_batch100_epoch2000.pickle\")\n",
    "    num_samples = 3000\n",
    "\n",
    "    # Rough calculation: 263 to 313 = 50, 263 to 343 = 80, 80*20 = 1600\n",
    "    # Generate PASS data: x5 is less than 313\n",
    "    pass_states = []\n",
    "    for i in range(num_samples):\n",
    "        # x0 to x19 can be anything in the range of 263 to 343 (from initial-10 to setpoint+10)\n",
    "        x0_x4 = np.random.randint(low=263, high=343, size=(5, ))\n",
    "        # x5 must be less than 313\n",
    "        x5 = np.random.randint(low=263, high=313)\n",
    "        x6_x19 = np.random.randint(low=263, high=343, size=(14,))\n",
    "        # Get one pass state, append to list of passed state - label is 1\n",
    "        state = np.hstack((x0_x4, x5, x6_x19)).flatten()\n",
    "        pass_states.append(state)\n",
    "\n",
    "    # Convert the passed states in the full state space to the reduced space\n",
    "    pass_states = np.array(pass_states)\n",
    "    df_cols = []\n",
    "    for i in range(20):\n",
    "        df_cols.append(\"x{}\".format(i))\n",
    "    pass_states_df = pd.DataFrame(pass_states, columns=df_cols)\n",
    "    print(pass_states_df)\n",
    "    rom_pass_states = autoencoder.encode(pass_states_df)\n",
    "\n",
    "    # Create a vector of ones to label the pass state\n",
    "    ones_vector = np.ones(shape=num_samples, dtype=int)\n",
    "    ones_vector_df = pd.DataFrame(ones_vector, columns=[\"pass\"])\n",
    "    # hstack pass dataframe with label\n",
    "    pass_df = pd.concat([rom_pass_states, ones_vector_df], axis=1)\n",
    "\n",
    "    # Generate FAIL data: x5 is more than 313\n",
    "    fail_states = []\n",
    "    for i in range(num_samples):\n",
    "        # x0 to x19 can be anything in the range of 263 to 343 (from initial-10 to setpoint+10)\n",
    "        x0_x4 = np.random.randint(low=263, high=343, size=(5, ))\n",
    "        # x5 fails if its more than 313\n",
    "        x5 = np.random.randint(low=314, high=333)\n",
    "        x6_x19 = np.random.randint(low=263, high=343, size=(14,))\n",
    "        # Get one fail state, append to list of failed states - label is 0\n",
    "        state = np.hstack((x0_x4, x5, x6_x19)).flatten()\n",
    "        fail_states.append(state)\n",
    "\n",
    "    # Convert the failed states in the full state space to the reduced space\n",
    "    fail_states = np.array(fail_states)\n",
    "    #\n",
    "    fail_states_df = pd.DataFrame(fail_states, columns=df_cols)\n",
    "    print(fail_states_df)\n",
    "    rom_fail_states = autoencoder.encode(fail_states_df)\n",
    "\n",
    "    # Create a vector of zeros to label the fail state\n",
    "    zeros_vector = np.zeros(shape=num_samples, dtype=int)\n",
    "    zeros_vector_df = pd.DataFrame(zeros_vector, columns=[\"pass\"])\n",
    "    # hstack pass dataframe with label\n",
    "    fail_df = pd.concat([rom_fail_states, zeros_vector_df], axis=1)\n",
    "\n",
    "    # vstack the SVM training data and save to csv\n",
    "    svm_training_data = pd.concat([pass_df, fail_df], axis=0)\n",
    "    svm_training_data.to_csv(\"svm_training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def run_svm():\n",
    "    training_df = pd.read_csv(\"svm_training_data.csv\")\n",
    "    x_y = training_df.to_numpy()\n",
    "    X = x_y[:, 1:4]\n",
    "    print(X)\n",
    "    Y = x_y[:, -1]\n",
    "    print(Y)\n",
    "    model = svm.SVC(kernel='linear', verbose=1)\n",
    "    clf = model.fit(X, Y)\n",
    "    # The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.\n",
    "    # Solve for w3 (z)\n",
    "    z = lambda x, y: (-clf.intercept_[0] - clf.coef_[0][0] * x - clf.coef_[0][1] * y) / clf.coef_[0][2]\n",
    "    tmp = np.linspace(-1.5, 2.5, 40)\n",
    "    x, y = np.meshgrid(tmp, tmp)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot3D(X[Y == 0, 0], X[Y == 0, 1], X[Y == 0, 2], 'ob')\n",
    "    ax.plot3D(X[Y == 1, 0], X[Y == 1, 1], X[Y == 1, 2], 'sr')\n",
    "    ax.plot_surface(x, y, z(x, y), alpha=0.2)\n",
    "    ax.set_xlim3d(-1, 2)\n",
    "    ax.set_ylim3d(-1, 2)\n",
    "    ax.set_zlim3d(-1, 2)\n",
    "    ax.view_init(0, -60)\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "    plt.savefig(\"svm_decision_boundary.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0195172  -0.46792153 -0.05973433]\n",
      " [ 1.0361204  -0.45224002  0.7710551 ]\n",
      " [ 1.2189639  -0.77077734 -0.10829385]\n",
      " ...\n",
      " [ 1.0265098  -0.5322219   0.03354688]\n",
      " [ 0.71588176 -0.365802    0.18486194]\n",
      " [ 1.2546597  -0.8108769  -0.08369084]]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # generate_data_for_svm()\n",
    "    run_svm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}